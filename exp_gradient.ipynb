{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        input_dim = 1\n",
    "        output_dim = 2\n",
    "        hidden_size = 200\n",
    "        layer_sizes = [input_dim, hidden_size, hidden_size]\n",
    "        layers = reduce(operator.add,\n",
    "            [[nn.Linear(a,b), nn.ReLU()]\n",
    "                for a,b in zip(layer_sizes[0:-1], layer_sizes[1:])])\n",
    "        layers += [nn.Linear(layer_sizes[-1], output_dim)]\n",
    "\n",
    "        for layer in layers:\n",
    "            if type(layer) == nn.Linear:\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self._net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._net(x)\n",
    "    \n",
    "    def apply_projection(self, f, A, b):\n",
    "        \"\"\"project f to satisfy Af<=b\"\"\"\n",
    "        return f - (torch.linalg.pinv(A) @ nn.ReLU()(A @ f[:,:,None] - b[:,:,None]))[:,:,0]\n",
    "\n",
    "def calc_Ab(x, boundary):\n",
    "    batch_size = x.size(dim=0)\n",
    "    A = torch.tensor([[0.0, 1.0]])\n",
    "    A = A.repeat(batch_size, 1, 1)\n",
    "    b = torch.tensor([boundary])\n",
    "    b= b.repeat(batch_size, 1)\n",
    "    return A, b\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Define a single training sample\n",
    "input_sample = torch.tensor([[0.0]])  # Input is 1D\n",
    "target_output = torch.tensor([[0.5, 0.2]])  # Target is 2D\n",
    "boundary = 0.5\n",
    "\n",
    "# Initialize a list to store outputs during training\n",
    "outputs = []\n",
    "outputs_proj = []\n",
    "\n",
    "# Perform training\n",
    "num_iterations = 20\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_sample)\n",
    "    A, b = calc_Ab(input_sample, boundary)\n",
    "    output_proj = model.apply_projection(output, A, b)\n",
    "    loss = criterion(output_proj, target_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    outputs.append(output.detach().numpy().flatten())\n",
    "    outputs_proj.append(output_proj.detach().numpy().flatten())\n",
    "\n",
    "# Convert outputs to a NumPy array for visualization\n",
    "outputs = np.array(outputs)\n",
    "outputs_proj = np.array(outputs_proj)\n",
    "\n",
    "# Visualize the gradient descent optimization\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.plot(outputs[:, 0], outputs[:, 1], marker='o', markersize=3, color='blue', ls=':', label=r'$f_\\theta$ Path')\n",
    "plt.scatter(outputs[0, 0], outputs[0, 1], facecolors='none', edgecolors='k', zorder=5)\n",
    "plt.plot(outputs_proj[:, 0], outputs_proj[:, 1], marker='o', markersize=3, color='orange', ls=':', label=r'$\\mathcal{P}(f_\\theta)$ Path')\n",
    "plt.scatter(outputs_proj[0, 0], outputs_proj[0, 1], facecolors='none', edgecolors='k', label=r'Initial Output', zorder=5)\n",
    "plt.scatter(target_output[0, 0].item(), target_output[0, 1].item(), color='red', label=r'Target Output', zorder=5)\n",
    "xlim = plt.gca().get_xlim()\n",
    "ylim = plt.gca().get_ylim()\n",
    "plt.fill_between(xlim, ylim[0], boundary, color='b', alpha=0.1, edgecolor='none', label='Feasible Set')\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel('Output Dimension 1')\n",
    "plt.ylabel('Output Dimension 2')\n",
    "plt.title('Gradient Descent Optimization Path')\n",
    "plt.legend(frameon='true')\n",
    "plt.grid(True)\n",
    "# plt.xlim([-10,10])\n",
    "# plt.ylim([-5,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import operator\n",
    "from functools import reduce\n",
    "\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        input_dim = 1\n",
    "        output_dim = 2\n",
    "        hidden_size = 10\n",
    "        layer_sizes = [input_dim, hidden_size, hidden_size]\n",
    "        layers = reduce(operator.add,\n",
    "            [[nn.Linear(a,b), nn.ReLU()]\n",
    "                for a,b in zip(layer_sizes[0:-1], layer_sizes[1:])])\n",
    "        layers += [nn.Linear(layer_sizes[-1], output_dim)]\n",
    "\n",
    "        for layer in layers:\n",
    "            if type(layer) == nn.Linear:\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        self._net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self._net(x)\n",
    "    \n",
    "    def apply_projection(self, f, A, b):\n",
    "        \"\"\"project f to satisfy Af<=b\"\"\"\n",
    "        return f - (torch.linalg.pinv(A) @ nn.ReLU()(A @ f[:,:,None] - b[:,:,None]))[:,:,0]\n",
    "\n",
    "def calc_Ab(x, boundary):\n",
    "    batch_size = x.size(dim=0)\n",
    "    A = torch.tensor([[0.0, 1.0]])\n",
    "    A = A.repeat(batch_size, 1, 1)\n",
    "    b = torch.tensor([boundary])\n",
    "    b= b.repeat(batch_size, 1)\n",
    "    return A, b\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "\n",
    "# Initialize the neural network, loss function, and optimizer\n",
    "model = SimpleNN()\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# Define a single training sample\n",
    "input_sample = torch.tensor([[-1.],[1.]])  # Input is 1D\n",
    "target_output = torch.tensor([[-.5, .5],\n",
    "                              [.5, 0.]])  # Target is 2D\n",
    "# input_sample = input_sample[0:1]\n",
    "# target_output = target_output[0:1]\n",
    "boundary = .9\n",
    "\n",
    "# Initialize a list to store outputs during training\n",
    "outputs = []\n",
    "outputs_proj = []\n",
    "\n",
    "# Perform training\n",
    "num_iterations = 100\n",
    "A, b = calc_Ab(input_sample, boundary)\n",
    "for i in range(num_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_sample)\n",
    "    output_proj = model.apply_projection(output, A, b)\n",
    "    loss = criterion(output_proj, target_output)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    outputs.append(output.detach().numpy())\n",
    "    outputs_proj.append(output_proj.detach().numpy())\n",
    "\n",
    "# Convert outputs to a NumPy array for visualization\n",
    "outputs = np.array(outputs)\n",
    "outputs_proj = np.array(outputs_proj)\n",
    "\n",
    "# Visualize the gradient descent optimization\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "for idx_input in range(len(input_sample)):\n",
    "    plt.plot(outputs[:, idx_input, 0], outputs[:, idx_input, 1], marker='o', fillstyle='none', markersize=4, color='blue', ls=':', label=r'$f_\\theta$ Path')\n",
    "    plt.scatter(outputs[0, idx_input, 0], outputs[0, idx_input, 1], facecolors='none', edgecolors='k', s=80, zorder=5)\n",
    "    plt.plot(outputs_proj[:, idx_input, 0], outputs_proj[:, idx_input, 1], marker='o', fillstyle='none', markersize=4, color='orange', ls=':', label=r'$\\mathcal{P}(f_\\theta)$ Path')\n",
    "    plt.scatter(outputs_proj[0, idx_input, 0], outputs_proj[0, idx_input, 1], facecolors='none', edgecolors='k', s=80, label=r'Initial Output', zorder=5)\n",
    "    plt.scatter(target_output[idx_input, 0].item(), target_output[idx_input, 1].item(), color='red', label=r'Target Output', zorder=5)\n",
    "# xlim = plt.gca().get_xlim()\n",
    "# ylim = plt.gca().get_ylim()\n",
    "xlim, ylim = [-2.5, 2.5], [-0.1, 1.4]\n",
    "plt.fill_between(xlim, ylim[0], boundary, color='b', alpha=0.1, edgecolor='none', label='Feasible Set')\n",
    "plt.xlim(xlim)\n",
    "plt.ylim(ylim)\n",
    "plt.xlabel('Output Dimension 1')\n",
    "plt.ylabel('Output Dimension 2')\n",
    "plt.title(f'Learning Rate = {lr}')\n",
    "# plt.legend(frameon='true')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# fig.savefig(f'results/figures/gd_two_lr{lr}.pdf',dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
